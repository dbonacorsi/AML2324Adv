{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 2","language":"python","name":"python2"},"language_info":{"codemirror_mode":{"name":"ipython","version":2},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython2","version":"2.7.16"},"colab":{"provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"Xfi_boTqVWl8"},"source":["# Ensemble Learning"]},{"cell_type":"markdown","metadata":{"id":"AdWCpGHmDSut"},"source":["## Import the data"]},{"cell_type":"code","metadata":{"id":"i-Dgw8lcDSiY"},"source":["import pandas as pd\n","\n","url = 'https://raw.githubusercontent.com/dbonacorsi/AML2021Bas/main/datasets/pima-indians-diabetes.data.csv'\n","\n","names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n","data = pd.read_csv(url, names=names)\n","data"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wYp3f9TFVWmB"},"source":["# BAGGING Algorithms"]},{"cell_type":"markdown","metadata":{"id":"RrsYfdpyVWmD"},"source":["## Bagged Decision Trees"]},{"cell_type":"markdown","metadata":{"id":"3EQBoeDnVWmE"},"source":["Bagging performs best with algorithms that have **high variance**."]},{"cell_type":"markdown","metadata":{"id":"nJKm2p5xCqiv"},"source":["\n","In the example below is an example of using the `BaggingClassifier` with the Classification and Regression Trees algorithm (`DecisionTreeClassifier` - more info [here](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html)). A total of 100 trees are created."]},{"cell_type":"code","metadata":{"id":"BFZfu9b7VWmF"},"source":["from sklearn.model_selection import KFold\n","from sklearn.model_selection import cross_val_score\n","#\n","from sklearn.ensemble import BaggingClassifier                     # <---\n","#\n","from sklearn.tree import DecisionTreeClassifier                    # <---"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n2KT2VmrVWmP"},"source":["array = data.values\n","X = array[:,0:8]\n","Y = array[:,8]\n","\n","seed = 42"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oPN-umJOGDs5"},"source":["Try a rough decision tree classifier."]},{"cell_type":"code","metadata":{"id":"Id7OxH-lFi9P"},"source":["kfold = KFold(n_splits=10, shuffle=True)\n","model = DecisionTreeClassifier(random_state=seed)\n","results = cross_val_score(model, X, Y, cv=kfold)\n","print(results.mean())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rMlrbk-5GHR1"},"source":["Then try to do better with Bagging."]},{"cell_type":"code","metadata":{"id":"k3zOVnWWVWmT"},"source":["# Bagged Decision Trees for Classification\n","kfold = KFold(n_splits=10, shuffle=True)\n","cart = DecisionTreeClassifier(random_state=seed)\n","num_trees = 100\n","model = BaggingClassifier(base_estimator=cart, n_estimators=num_trees, random_state=seed, bootstrap = True)\n","results = cross_val_score(model, X, Y, cv=kfold)\n","print(results.mean())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BVLCmZvjVWmd"},"source":["*(NOTE: running the cell above should take some more time than usual..)*"]},{"cell_type":"markdown","metadata":{"id":"YuYthstlJEHf"},"source":["Now, try to change one parameter above, `bootstrap = True` and rerun and see what happens..\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ZcSfpxJjVWme"},"source":["Running the example in the latter way, we get a more robust estimate of model accuracy."]},{"cell_type":"markdown","metadata":{"id":"S5WVLm-5VWmf"},"source":["## Random Forest"]},{"cell_type":"markdown","metadata":{"id":"zSjdZZdoVWmf"},"source":["Random Forests is **an extension of bagged decision trees**."]},{"cell_type":"markdown","metadata":{"id":"f_mFGJLPD2-D"},"source":["\n","You can construct a Random Forest model for classification using the RandomForestClassifier class, documented [here](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html). The example below demonstrates using Random Forest for classification with 100 trees and split points chosen from a random selection of 3 features."]},{"cell_type":"code","metadata":{"id":"-O3AyOSCVWmg"},"source":["from sklearn.ensemble import RandomForestClassifier                    # <---"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5-EqH7WyVWmj"},"source":["# Random Forest Classification\n","num_trees = 100\n","max_features = 3\n","kfold = KFold(n_splits=10, shuffle=True)\n","model = RandomForestClassifier(n_estimators=num_trees, max_features=max_features)\n","results = cross_val_score(model, X, Y, cv=kfold)\n","print(results.mean())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TopM7bMtVWmn"},"source":["*(NOTE: running the cell above should take some more time than usual..)*"]},{"cell_type":"markdown","metadata":{"id":"bc5F21QnVWmt"},"source":["Running the example provides a mean estimate of classification accuracy."]},{"cell_type":"markdown","metadata":{"id":"et8Lty-OVWmu"},"source":["## Extra Trees"]},{"cell_type":"markdown","metadata":{"id":"zeXTTykSVWmv"},"source":["Extra Trees are **another modification of bagging** where random trees are constructed from samples of the training dataset.\n","\n","You can construct an Extra Trees model for classification using the ExtraTreesClassifier class (documented [here](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html)).\n","\n","The example below provides a demonstration of extra trees with the number of trees set to 100 and splits chosen from 7 random features."]},{"cell_type":"code","metadata":{"id":"JbCOK42pVWmw"},"source":["from sklearn.ensemble import ExtraTreesClassifier                    # <---"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VS7gQwjfVWmz"},"source":["# Extra Trees Classification\n","num_trees = 100\n","max_features = 7\n","kfold = KFold(n_splits=10, shuffle=True)\n","model = ExtraTreesClassifier(n_estimators=num_trees, max_features=max_features)\n","results = cross_val_score(model, X, Y, cv=kfold)\n","print(results.mean())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pk536J6YVWm4"},"source":["*(NOTE: running the cell above should take some more time than usual..)*"]},{"cell_type":"markdown","metadata":{"id":"akWQIdjkVWm9"},"source":["Running the example provides a mean estimate of classification accuracy."]},{"cell_type":"markdown","metadata":{"id":"UKzwjkvhVWnB"},"source":["# BOOSTING Algorithms"]},{"cell_type":"markdown","metadata":{"id":"jjviT5glVWnD"},"source":["## AdaBoost"]},{"cell_type":"markdown","metadata":{"id":"9v1loEZRVWnE"},"source":["You can construct an AdaBoost model for classification using the AdaBoostClassifier class (documented [here](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html)).\n","\n","The example below demonstrates the construction of 30 decision trees in sequence using the AdaBoost algorithm."]},{"cell_type":"code","metadata":{"id":"NmemHOa8VWnF"},"source":["from sklearn.ensemble import AdaBoostClassifier                    # <---"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"QUjCOmBUVWnP"},"source":["# AdaBoost Classification\n","num_trees = 30\n","kfold = KFold(n_splits=10, shuffle=True)\n","model = AdaBoostClassifier(n_estimators=num_trees, random_state=seed)\n","results = cross_val_score(model, X, Y, cv=kfold)\n","print(results.mean())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ONOC1TqhVWnT"},"source":["Running the example provides a mean estimate of classification accuracy."]},{"cell_type":"markdown","metadata":{"id":"7R464WKDVWnU"},"source":["## Stochastic Gradient Boosting"]},{"cell_type":"markdown","metadata":{"id":"vEFqIfx6VWnU"},"source":["You can construct a Gradient Boosting model for classification using the `GradientBoostingClassifier` class (documented [here](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html)).\n","\n","The example below demonstrates Stochastic Gradient Boosting for classification with 100 trees."]},{"cell_type":"code","metadata":{"id":"hsa9mZwpVWnV"},"source":["from sklearn.ensemble import GradientBoostingClassifier                    # <---"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"RLwk7B_HVWnY"},"source":["# Stochastic Gradient Boosting Classification\n","num_trees = 100\n","kfold = KFold(n_splits=10, shuffle=True)\n","model = GradientBoostingClassifier(n_estimators=num_trees, random_state=seed)\n","results = cross_val_score(model, X, Y, cv=kfold)\n","print(results.mean())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"188VZWg6VWni"},"source":["Running the example provides a mean estimate of classification accuracy."]},{"cell_type":"markdown","metadata":{"id":"B6Wt_X83VWnj"},"source":["# VOTING Algorithms"]},{"cell_type":"markdown","metadata":{"id":"iD2zgBiIVWnj"},"source":["Voting remains **one of the simplest ways of combining the predictions from multiple ML algorithms**."]},{"cell_type":"markdown","metadata":{"id":"92SBXBnNMWLK"},"source":["\n","You can create a voting ensemble model for classification using the `VotingClassifier` class (documented [here](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html)).\n","\n","The code below provides an example of combining the predictions of logistic regression, classification and regression trees and support vector machines together for a classification problem.\n"]},{"cell_type":"code","metadata":{"id":"y5KugbTdVWnk"},"source":["from sklearn.linear_model import LogisticRegression\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.svm import SVC\n","from sklearn.ensemble import VotingClassifier                    # <---"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GUbFImQwVWnp"},"source":["# Voting Ensemble for Classification\n","\n","kfold = KFold(n_splits=10, shuffle=True)\n","\n","# create the sub models\n","estimators = []\n","#model1 = LogisticRegression()\n","model1 = LogisticRegression(C=10, tol=0.01, solver='lbfgs', max_iter=10000)\n","\n","estimators.append(( 'logistic' , model1))\n","model2 = DecisionTreeClassifier()\n","estimators.append(( 'cart' , model2))\n","model3 = SVC()\n","estimators.append(( 'svm' , model3))\n","\n","# create the ensemble model\n","ensemble = VotingClassifier(estimators)\n","results = cross_val_score(ensemble, X, Y, cv=kfold)\n","print(results.mean())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GX5uXJusVWnz"},"source":["Running the example provides a mean estimate of classification accuracy."]},{"cell_type":"markdown","metadata":{"id":"Ofoq2FdTVWn0"},"source":["## Summary"]},{"cell_type":"markdown","metadata":{"id":"h2GI3jPVVWn0"},"source":["What we did:\n","\n","* we discovered ensemble ML algorithms for improving the performance of models on your problems. You learned about\n","Bagging Ensembles including Bagged Decision Trees, Random Forest and Extra Trees, Boosting Ensembles including AdaBoost and Stochastic Gradient Boosting, Voting Ensembles for averaging the predictions for any arbitrary models."]},{"cell_type":"markdown","metadata":{"id":"wOmHj5v-qdbC"},"source":["# Exercises"]},{"cell_type":"markdown","metadata":{"id":"8x0ewK8xqZwa"},"source":["## <font color=red>Exercise 1</font>"]},{"cell_type":"markdown","metadata":{"id":"vansAqF_qtoq"},"source":["1. Load the *MNIST* data and split it into a training set, a validation set, and a test set (e.g. use 50k instances for training, and 10k each for validation and testing).\n","2. Then train various classifiers, such as a **Random Forest** classifier, an **Extra-Trees** classifier, and an **SVM** classifier, for example. If you want, pick more.\n","3. Next, try to combine them into an **ensemble** that outperforms each individual classifier on the validation set, using **soft or hard voting**.\n","4. Once you have found one, try it on the test set. How much better does it perform compared to the individual classifiers?"]},{"cell_type":"markdown","metadata":{"id":"5jlcEG4xqy5r"},"source":["### <font color='green'>Solution</font>"]},{"cell_type":"code","metadata":{"id":"nRYBbyb0q18D"},"source":["# type your code below"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"17S0EDpAq-S_"},"source":["_Credits: Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow (2nd Edition) by Aurélien Géron, O'Reilly Media Inc., 2019_"]}]}